"""
æ¯å‘¨éŠæˆ²è¡ŒéŠ·æ‘˜è¦æ¨¡çµ„
- ç›®æ¨™éŠæˆ²ï¼šAndroid ç‡Ÿæ”¶ Top 10 + å·´å“ˆç†±é–€ç‰ˆ Top 10ï¼ˆåˆä½µå»é‡ï¼‰
- è³‡æ–™ä¾†æºï¼šGoogle News RSS + 4Gamers tag + YouTube Data API + å·´å“ˆéŠæˆ²æ¿å…¬å‘Š
- åˆ†é¡ï¼šğŸ“¢ å»£å‘Š/è¡ŒéŠ· â”‚ ğŸ‰ æ´»å‹• â”‚ ğŸ¤ è¯ååˆä½œ
- æ™‚é–“ç¯„åœï¼šéå» 14 å¤©ï¼ˆæ¶µè“‹é€²è¡Œä¸­æ´»å‹•ï¼‰
- æ’ç¨‹ï¼šæ¯å‘¨ä¸€åŸ·è¡Œä¸€æ¬¡
"""
import httpx
from bs4 import BeautifulSoup
from collections import Counter
import feedparser
import json
import os
import sys
import time
import re
import urllib.parse
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime

TW_TZ = timezone(timedelta(hours=8))


def _log(msg: str):
    """Safe print for Windows cp950 terminal"""
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode(sys.stdout.encoding or "utf-8", errors="replace").decode(
            sys.stdout.encoding or "utf-8", errors="replace"))


CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "cache")
CACHE_FILE = os.path.join(CACHE_DIR, "weekly_digest.json")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8",
}

# â”€â”€ åˆ†é¡é—œéµå­— â”€â”€
EVENT_KEYWORDS = [
    "æ´»å‹•", "é™å®š", "é–‹è·‘", "ç™»å ´", "é–‹æ”¾", "æ›´æ–°", "æ”¹ç‰ˆ", "ç‰ˆæœ¬", "è³½å­£",
    "ç¯€æ…¶", "å‘¨å¹´", "æ˜¥ç¯€", "éå¹´", "æ–°å¹´", "ç¶­è­·", "å…¬å‘Š", "çå‹µ", "å„²å€¼",
    "è½‰è›‹", "æŠ½ç", "å…è²»", "è´ˆé€",
]
COLLAB_KEYWORDS = ["åˆä½œ", "è¯å", "è¯å‹•", "é€£å‹•", "è·¨ç•Œ", "x ", "Ã—", "æ”œæ‰‹", "æˆæ¬Š"]
AD_KEYWORDS = [
    "å»£å‘Š", "ä»£è¨€", "å¤§ä½¿", "å®£å‚³", "PV", "CM", "é å‘Š", "trailer",
    "MV", "å½¢è±¡", "å“ç‰Œ", "å®˜æ–¹", "ä¸»é¡Œæ›²", "è´ŠåŠ©", "æ¨å»£", "KOL",
]

# â”€â”€ ééŠæˆ²é»‘åå–®ï¼ˆå·´å“ˆå§†ç‰¹ç†±é–€ç‰ˆä¸­çš„ééŠæˆ²æ¿ï¼‰â”€â”€
BOARD_BLACKLIST = [
    "é›»è…¦æ‡‰ç”¨ç¶œåˆè¨è«–", "å ´å¤–ä¼‘æ†©å€", "å“ˆå•¦æ¿å‹™", "å‹•æ¼«æˆ²åŠ‡ç¶œåˆ",
    "æ™ºæ…§å‹æ‰‹æ©Ÿ", "é›»è…¦ç¡¬é«”", "æ¨¡å‹å…¬ä»”", "ç”Ÿæ´»å¨›æ¨‚",
]

# â”€â”€ åšå¼ˆ/ééŠæˆ² App é—œéµå­—éæ¿¾ â”€â”€
GAME_NAME_BLACKLIST_KW = ["å¨›æ¨‚åŸ", "éº»å°‡", "è€è™æ©Ÿ", "åˆ®åˆ®æ¨‚", "åšå¼ˆ", "æ£‹ç‰Œ"]

# â”€â”€ éŠæˆ²åç¨±æ¸…ç†ï¼šå»å‰ç¶´/å‰¯æ¨™é¡Œï¼Œå–æ ¸å¿ƒåç¨± â”€â”€
NAME_PREFIXES = ["Garena ", "SEGA ", "miHoYo ", "Netmarble "]

def _clean_game_name(raw_name: str) -> str:
    """æ¸…ç†éŠæˆ²åç¨±ï¼šå»æ‰ç™¼è¡Œå•†å‰ç¶´å’Œå‰¯æ¨™é¡Œ"""
    name = raw_name.strip()
    # å»æ‰ç™¼è¡Œå•†å‰ç¶´
    for prefix in NAME_PREFIXES:
        if name.startswith(prefix):
            name = name[len(prefix):]
            break
    # å»æ‰å‰¯æ¨™é¡Œï¼ˆï¼šæˆ– - å¾Œçš„æè¿°æ€§æ–‡å­—ï¼‰
    for sep in ["ï¼š", " - ", "â€”"]:
        if sep in name:
            base = name.split(sep)[0].strip()
            # ä¿ç•™æœ‰æ„ç¾©çš„çŸ­åï¼ˆè‡³å°‘ 2 å­—ï¼‰
            if len(base) >= 2:
                name = base
                break
    return name

# â”€â”€ 4Gamers tag åç¨±å°ç…§è¡¨ â”€â”€
TAG_ALIASES = {
    "å‹åˆ©å¥³ç¥ï¼šå¦®å§¬": ["NIKKE", "å‹åˆ©å¥³ç¥"],
    "å´©å£ï¼šæ˜Ÿç©¹éµé“": ["æ˜Ÿç©¹éµé“", "å´©å£æ˜Ÿç©¹éµé“"],
    "è”šè—æª”æ¡ˆ Blue Archive": ["è”šè—æª”æ¡ˆ", "Blue Archive"],
    "Fate/Grand Order": ["FGO", "Fate"],
    "å“ˆåˆ©æ³¢ç‰¹ï¼šé­”æ³•è¦ºé†’": ["å“ˆåˆ©æ³¢ç‰¹"],
    "æ˜æ—¥æ–¹èˆŸï¼šçµ‚æœ«åœ°": ["æ˜æ—¥æ–¹èˆŸ", "Arknights"],
    "å‚³èªªå°æ±º": ["AOV", "Arena of Valor"],
    "å¤©å ‚W": ["å¤©å ‚", "Lineage"],
    "åŸç¥": ["Genshin", "Genshin Impact"],
    "ç¥é­”ä¹‹å¡”": ["Tower of Saviors"],
    "å¤©å ‚M": ["Lineage M", "å¤©å ‚ Mobile"],
    "ROä»™å¢ƒå‚³èª¬": ["ä»™å¢ƒå‚³èªª", "RO"],
    "è²“å’ªå¤§æˆ°çˆ­": ["Battle Cats"],
    "æ˜ŸåŸOnline": ["æ˜ŸåŸ"],
}

# â”€â”€ å¸¸è¦‹æ‰‹éŠ BSN å°ç…§ï¼ˆè£œå·´å“ˆç†±é–€ç‰ˆæœªæ¶µè“‹çš„éŠæˆ²ï¼‰â”€â”€
KNOWN_BSN = {
    "å‚³èªªå°æ±º": "30518",
    "å¤©å ‚M": "25908",
    "å¯’éœœå•Ÿç¤ºéŒ„": "76999",
    "Kingshot": "82382",
    "ROä»™å¢ƒå‚³èª¬": "28924",
    "æœ€å¾Œçš„æˆ°çˆ­": "79869",
    "è²“å’ªå¤§æˆ°çˆ­": "23772",
    "ç¥é­”ä¹‹å¡”": "23805",
    "åŸç¥": "36730",
    "å´©å£ï¼šæ˜Ÿç©¹éµé“": "75165",
    "å‹åˆ©å¥³ç¥ï¼šå¦®å§¬": "74498",
    "æ˜æ—¥æ–¹èˆŸï¼šçµ‚æœ«åœ°": "74604",
    "è”šè—æª”æ¡ˆ Blue Archive": "73498",
}


async def _search_bsn(client: httpx.AsyncClient, game_name: str) -> str | None:
    """è‡ªå‹•æœå°‹å·´å“ˆå§†ç‰¹éŠæˆ²æ¿ BSNï¼šACG æœå°‹ + æ¿é æ¨™é¡Œé©—è­‰"""
    encoded = urllib.parse.quote(game_name)
    try:
        resp = await client.get(
            f"https://acg.gamer.com.tw/search.php?s=3&kw={encoded}",
            timeout=15,
        )
        if resp.status_code != 200:
            return None
    except Exception:
        return None

    bsn_list = re.findall(r'(?:G2|C|B)\.php\?bsn=0*(\d+)', resp.text)
    counter = Counter(bsn_list)
    candidates = [bsn for bsn, _ in counter.most_common(5)]
    if not candidates:
        return None

    # ç”¢ç”Ÿæ‰€æœ‰å¯èƒ½çš„åŒ¹é…åç¨±ï¼ˆå« TAG_ALIASES è®Šé«” + CJK baseï¼‰
    match_names = {game_name.lower()}
    for canonical, aliases in TAG_ALIASES.items():
        all_names = [canonical] + aliases
        if any(n in game_name or game_name in n for n in all_names):
            for n in all_names:
                match_names.add(n.lower())
            break
    cjk_base = re.sub(r'[^\u4e00-\u9fff]', '', game_name)
    if len(cjk_base) >= 2:
        match_names.add(cjk_base)

    # é€ä¸€é©—è­‰å€™é¸ BSNï¼šæ¿é æ¨™é¡Œ/æè¿°å¿…é ˆåŒ…å«éŠæˆ²åç¨±
    for bsn in candidates:
        try:
            resp2 = await client.get(
                f"https://forum.gamer.com.tw/B.php?bsn={bsn}",
                timeout=10,
            )
            if resp2.status_code != 200:
                continue
            soup = BeautifulSoup(resp2.text, "html.parser")
            title_el = soup.select_one("title")
            page_title = (title_el.get_text() if title_el else "").lower()
            meta = soup.select_one('meta[name="description"]')
            desc = (meta.get("content", "") if meta else "").lower()
            full = page_title + " " + desc

            if any(name in full for name in match_names):
                _log(f"[WeeklyDigest] Auto-BSN: {game_name} -> bsn={bsn}")
                return bsn
        except Exception:
            continue
    return None


def _title_contains_game(title: str, game_name: str) -> bool:
    """æª¢æŸ¥æ¨™é¡Œæ˜¯å¦åŒ…å«éŠæˆ²åç¨±ï¼ˆå«åˆ¥å + CJK æ ¸å¿ƒå­—ï¼‰"""
    t = title.lower()
    # ä¸»åç¨±
    if game_name.lower() in t:
        return True
    # TAG_ALIASES åˆ¥å
    for canonical, aliases in TAG_ALIASES.items():
        all_names = [canonical] + aliases
        if any(n.lower() in game_name.lower() or game_name.lower() in n.lower() for n in all_names):
            if any(alias.lower() in t for alias in all_names):
                return True
            break
    # CJK æ ¸å¿ƒå­—ï¼ˆè‡³å°‘ 2 å­—çš„ä¸­æ–‡éƒ¨åˆ†ï¼‰
    cjk = re.sub(r'[^\u4e00-\u9fff]', '', game_name)
    if len(cjk) >= 2 and cjk in title:
        return True
    return False


def _classify_item(title: str, summary: str = "") -> list[str]:
    """æ ¹æ“šæ¨™é¡Œå’Œæ‘˜è¦åˆ†é¡æ¶ˆæ¯é¡å‹"""
    text = f"{title} {summary}".lower()
    tags = []
    if any(kw in text for kw in AD_KEYWORDS):
        tags.append("ad")
    if any(kw in text for kw in COLLAB_KEYWORDS):
        tags.append("collab")
    if any(kw in text for kw in EVENT_KEYWORDS):
        tags.append("event")
    if not tags:
        tags.append("news")
    return tags


def _get_search_range():
    """å–å¾—æœå°‹æ™‚é–“ç¯„åœï¼šéå» 14 å¤©ï¼ˆæ¶µè“‹é€²è¡Œä¸­çš„æ´»å‹•ï¼‰"""
    now = datetime.now(TW_TZ)
    start = now - timedelta(days=14)
    return start, now


async def _get_target_games() -> list[dict]:
    """
    å¾ç¾æœ‰å¿«å–å–å¾—ç›®æ¨™éŠæˆ²æ¸…å–®ï¼š
    - Android ç‡Ÿæ”¶ Top 10
    - å·´å“ˆå§†ç‰¹ç†±é–€ç‰ˆ Top 10ï¼ˆå« bsnï¼‰
    åˆä½µå»é‡å¾Œå›å‚³
    """
    games = []
    seen_names = set()

    # 1. Android ç‡Ÿæ”¶ Top 10
    mobile_cache = os.path.join(CACHE_DIR, "mobile_data.json")
    try:
        with open(mobile_cache, "r", encoding="utf-8") as f:
            mobile_data = json.load(f)
        android_grossing = mobile_data.get("android", {}).get("grossing", [])
        for item in android_grossing[:10]:
            raw_name = item.get("name", "").strip()
            if not raw_name:
                continue
            # éæ¿¾åšå¼ˆé¡
            if any(kw in raw_name for kw in GAME_NAME_BLACKLIST_KW):
                continue
            name = _clean_game_name(raw_name)
            if name and name not in seen_names:
                seen_names.add(name)
                games.append({
                    "name": name,
                    "source": "android_grossing",
                    "rank": item.get("rank", 0),
                    "bsn": None,
                })
    except (FileNotFoundError, json.JSONDecodeError, KeyError):
        _log("[WeeklyDigest] Mobile cache not found, skipping Android")

    # 2. å·´å“ˆå§†ç‰¹ç†±é–€ç‰ˆ Top 10ï¼ˆå« bsnï¼‰
    discussion_cache = os.path.join(CACHE_DIR, "discussion_data.json")
    try:
        with open(discussion_cache, "r", encoding="utf-8") as f:
            disc_data = json.load(f)
        bahamut_boards = disc_data.get("bahamut_boards", [])

        # å»ºç«‹ bsn å°ç…§è¡¨ï¼Œä¹Ÿå˜—è©¦å¹« Android éŠæˆ²è£œä¸Š bsn
        bsn_map = {b.get("name", ""): b.get("bsn", "") for b in bahamut_boards}

        for item in bahamut_boards[:10]:
            name = item.get("name", "").strip()
            if not name:
                continue
            # éæ¿¾ééŠæˆ²æ¿
            if name in BOARD_BLACKLIST:
                continue
            if name not in seen_names:
                seen_names.add(name)
                games.append({
                    "name": name,
                    "source": "bahamut_hot",
                    "rank": item.get("rank", 0),
                    "bsn": item.get("bsn"),
                })

        # å¹«å·²æœ‰çš„ Android éŠæˆ²è£œ bsnï¼ˆåç¨±æ¨¡ç³ŠåŒ¹é…ï¼‰
        for game in games:
            if game["bsn"] is None:
                for board_name, bsn in bsn_map.items():
                    if game["name"] in board_name or board_name in game["name"]:
                        game["bsn"] = bsn
                        break
    except (FileNotFoundError, json.JSONDecodeError, KeyError):
        _log("[WeeklyDigest] Discussion cache not found, skipping Bahamut")

    # 3. è‡ªå‹•æœå°‹å·´å“ˆ BSNï¼ˆACG æœå°‹ + æ¿é é©—è­‰ï¼‰
    missing_bsn = [g for g in games if g["bsn"] is None]
    if missing_bsn:
        async with httpx.AsyncClient(timeout=15, follow_redirects=True, headers=HEADERS) as client:
            for game in missing_bsn:
                found = await _search_bsn(client, game["name"])
                if found:
                    game["bsn"] = found

    # 4. ç”¨ KNOWN_BSN ç¡¬ç·¨ç¢¼è£œä¸Šä»ç¼º bsn çš„éŠæˆ²ï¼ˆæœ€çµ‚å…œåº•ï¼‰
    for game in games:
        if game["bsn"] is None:
            for known_name, known_bsn in KNOWN_BSN.items():
                if game["name"] in known_name or known_name in game["name"]:
                    game["bsn"] = known_bsn
                    break

    with_bsn = len([g for g in games if g["bsn"]])
    _log(f"[WeeklyDigest] Target games: {len(games)} "
         f"({len([g for g in games if g['source'] == 'android_grossing'])} Android + "
         f"{len([g for g in games if g['source'] == 'bahamut_hot'])} Bahamut), "
         f"{with_bsn} with BSN")
    return games


def _get_tag_variants(game_name: str) -> list[str]:
    """å–å¾—éŠæˆ²åç¨±çš„æ‰€æœ‰å¯èƒ½ tag è®Šé«”ï¼ˆæ”¯æ´å­å­—ä¸²åŒ¹é…ï¼‰"""
    variants = [game_name]
    for canonical, aliases in TAG_ALIASES.items():
        all_names = [canonical] + aliases
        # å­å­—ä¸²åŒ¹é…ï¼šéŠæˆ²åç¨±åŒ…å« canonical/aliasï¼Œæˆ–åéä¾†
        if any(n in game_name or game_name in n for n in all_names):
            variants = [canonical] + aliases + [game_name]
            break
    return list(set(variants))


# ============================================================
# ä¾†æº 1: 4Gamers tag æœå°‹
# ============================================================
async def _search_4gamers(client: httpx.AsyncClient, game_name: str, since: datetime) -> list[dict]:
    """æœå°‹ 4Gamers ç‰¹å®šéŠæˆ²çš„è¿‘æœŸæ–°è"""
    items = []
    variants = _get_tag_variants(game_name)

    for tag in variants:
        encoded = urllib.parse.quote(tag)
        url = f"https://www.4gamers.com.tw/site/api/news/by-tag?tag={encoded}&pageSize=20"
        try:
            resp = await client.get(url, headers=HEADERS)
            if resp.status_code != 200 or "json" not in resp.headers.get("content-type", ""):
                continue
            data = resp.json()
            items = data.get("data", {}).get("results", [])
            if items:
                break
        except Exception:
            continue
    else:
        return []

    since_ts = int(since.timestamp() * 1000)
    results = []
    for item in items:
        ts = item.get("createPublishedAt", 0)
        if ts < since_ts:
            continue
        title = item.get("title", "")
        intro = item.get("intro", "") or ""
        results.append({
            "title": title,
            "url": item.get("canonicalUrl", ""),
            "summary": intro[:120],
            "source": "4Gamers",
            "published_at": time.strftime("%Y-%m-%dT%H:%M:%S", time.gmtime(ts / 1000)),
            "tags": _classify_item(title, intro),
        })

    return results


# ============================================================
# ä¾†æº 2: YouTube Data API â€” å®˜æ–¹å½±éŸ³/å»£å‘Š/PV
# ============================================================
async def _search_youtube(client: httpx.AsyncClient, game_name: str, since: datetime) -> list[dict]:
    """æœå°‹ YouTube ä¸Šçš„éŠæˆ²å®˜æ–¹å½±éŸ³/å»£å‘Š"""
    api_key = os.getenv("YOUTUBE_API_KEY", "")
    if not api_key:
        return []

    results = []
    # èšç„¦è¡ŒéŠ·ç›¸é—œæœå°‹ï¼ˆæ´»å‹•/è¯å/å»£å‘Šï¼‰ï¼Œä¸æœã€Œå®˜æ–¹ã€é¿å…æ‹‰åˆ°ä¸€èˆ¬å½±ç‰‡
    queries = [
        f"{game_name} æ´»å‹• è¯å",
        f"{game_name} å»£å‘Š PV CM",
    ]
    published_after = since.strftime("%Y-%m-%dT%H:%M:%SZ")

    # æ’é™¤éè¡ŒéŠ·å…§å®¹
    yt_skip_words = [
        "å¯¦æ³", "ç›´æ’­", "æ”»ç•¥", "æ•™å­¸", "é–‹ç®±", "å¿ƒå¾—", "è©•æ¸¬", "review",
        "gameplay", "walkthrough", "let's play", "åˆ†äº«", "è©¦ç©", "é«”é©—",
        "æ¯”è¼ƒ", "æ¨è–¦", "tier list", "é€šé—œ", "æŒ‘æˆ°", "æŠ½å¡", "èª²é‡‘",
        "pvp", "pve", "çµ„éšŠ", "é…è£", "æ‡¶äººåŒ…",
    ]

    for q in queries:
        try:
            resp = await client.get(
                "https://www.googleapis.com/youtube/v3/search",
                params={
                    "part": "snippet",
                    "q": q,
                    "type": "video",
                    "publishedAfter": published_after,
                    "regionCode": "TW",
                    "relevanceLanguage": "zh-Hant",
                    "maxResults": 5,
                    "order": "relevance",
                    "key": api_key,
                },
                timeout=15,
            )
            if resp.status_code != 200:
                _log(f"[WeeklyDigest] YouTube API error {resp.status_code} for '{q}'")
                continue
            data = resp.json()

            for item in data.get("items", []):
                snippet = item.get("snippet", {})
                title = snippet.get("title", "")
                channel = snippet.get("channelTitle", "")
                video_id = item.get("id", {}).get("videoId", "")
                published = snippet.get("publishedAt", "")

                # æ¨™é¡Œå¿…é ˆåŒ…å«éŠæˆ²åç¨±ï¼ˆé˜²æ­¢ YT æ¨è–¦ç„¡é—œå½±ç‰‡ï¼‰
                if not _title_contains_game(title, game_name):
                    continue

                title_lower = title.lower()
                # æ’é™¤æ”»ç•¥/å¯¦æ³é¡
                if any(sw in title_lower for sw in yt_skip_words):
                    continue

                # å¿…é ˆåŒ…å«è‡³å°‘ä¸€å€‹è¡ŒéŠ·ç›¸é—œé—œéµå­—
                marketing_kws = EVENT_KEYWORDS + COLLAB_KEYWORDS + AD_KEYWORDS
                if not any(kw.lower() in title_lower for kw in marketing_kws):
                    continue

                results.append({
                    "title": title,
                    "url": f"https://www.youtube.com/watch?v={video_id}",
                    "summary": f"é »é“ï¼š{channel}",
                    "source": "YouTube",
                    "published_at": published[:19] if published else "",
                    "tags": _classify_item(title),
                    "thumbnail": snippet.get("thumbnails", {}).get("medium", {}).get("url", ""),
                })
        except Exception as e:
            _log(f"[WeeklyDigest] YouTube search error: {e}")

    # å»é‡
    seen = set()
    unique = []
    for r in results:
        if r["url"] not in seen:
            seen.add(r["url"])
            unique.append(r)
    return unique


# ============================================================
# ä¾†æº 3: å·´å“ˆå§†ç‰¹éŠæˆ²æ¿ â€” æ´»å‹•/å…¬å‘Š/å®˜æ–¹è²¼æ–‡
# ============================================================
async def _search_bahamut_board(client: httpx.AsyncClient, bsn: str, game_name: str) -> list[dict]:
    """æœå°‹å·´å“ˆå§†ç‰¹éŠæˆ²æ¿ä¸Šçš„æ´»å‹•/å…¬å‘Š/å®˜æ–¹ç›¸é—œè²¼æ–‡"""
    if not bsn:
        return []

    url = f"https://forum.gamer.com.tw/B.php?bsn={bsn}"
    try:
        resp = await client.get(url, headers=HEADERS)
        if resp.status_code != 200:
            return []

        soup = BeautifulSoup(resp.text, "html.parser")
        results = []
        seen_titles = set()

        # åªä¿ç•™æƒ…å ±/å…¬å‘Šé¡è²¼æ–‡å‰ç¶´
        allow_prefixes = ["ã€æƒ…å ±ã€‘", "ã€å…¬å‘Šã€‘", "ã€å®˜æ–¹ã€‘", "ç²¾è¯"]
        # æ’é™¤æ”»ç•¥/å¿ƒå¾—/é–’èŠ/å•é¡Œ
        deny_prefixes = ["ã€å¿ƒå¾—ã€‘", "ã€æ”»ç•¥ã€‘", "ã€é–’èŠã€‘", "ã€å•é¡Œã€‘", "ã€å¯†æŠ€ã€‘", "ã€è¨è«–ã€‘"]

        # è¡ŒéŠ·ç›¸é—œé—œéµå­—ï¼ˆtitle å¿…é ˆåŒ…å«è‡³å°‘ä¸€å€‹ï¼‰
        # æ³¨æ„ï¼šã€Œæ›´æ–°ã€ã€Œå…¬å‘Šã€ã€Œç¶­è­·ã€å¤ªæ³›ï¼Œæœƒæ‹‰å…¥ç´”éŠæˆ²å…¬å‘Šï¼Œä¸å±¬æ–¼è¡ŒéŠ·
        marketing_kws = [
            "æ´»å‹•", "è¯å", "åˆä½œ", "é™å®š", "è¯å‹•", "é€£å‹•", "è·¨ç•Œ",
            "é–‹è·‘", "çå‹µ", "è´ˆé€", "å…è²»", "é å‘Š", "è³½äº‹", "ä»£è¨€",
            "å»£å‘Š", "PV", "ä¸»é¡Œæ›²", "è´ŠåŠ©", "å‘¨å¹´", "é€±å¹´", "ç¯€æ…¶",
            "æ˜¥ç¯€", "æ–°å¹´", "é€ å‹", "å„²å€¼", "æŠ½ç",
        ]

        for a in soup.select("a[href]"):
            href = a.get("href", "")
            if "C.php?bsn=" not in href:
                continue

            title = a.get_text(strip=True)
            if not title or len(title) < 5 or title in seen_titles:
                continue

            # æ’é™¤å¿ƒå¾—/æ”»ç•¥/é–’èŠ
            if any(title.startswith(p) for p in deny_prefixes):
                continue

            # å¿…é ˆæ˜¯æƒ…å ±/å…¬å‘Šé¡ï¼Œæˆ–åŒ…å«è¡ŒéŠ·é—œéµå­—
            is_info_post = any(title.startswith(p) for p in allow_prefixes)
            has_marketing_kw = any(kw in title for kw in marketing_kws)
            if not is_info_post and not has_marketing_kw:
                continue

            # å³ä½¿æ˜¯æƒ…å ±è²¼ï¼Œä¹Ÿè¦æœ‰è¡ŒéŠ·å…§å®¹ï¼ˆæ’é™¤ç´”æ•¸æ“š/æ’è¡Œæƒ…å ±ï¼‰
            if is_info_post and not has_marketing_kw:
                continue

            seen_titles.add(title)
            if not href.startswith("http"):
                href = f"https://forum.gamer.com.tw/{href}"

            results.append({
                "title": title,
                "url": href,
                "summary": f"å·´å“ˆ {game_name} æ¿",
                "source": "å·´å“ˆè¨è«–æ¿",
                "published_at": "",
                "tags": _classify_item(title),
            })

            if len(results) >= 8:
                break

        return results
    except Exception as e:
        _log(f"[WeeklyDigest] Bahamut board error for bsn={bsn}: {e}")
        return []


# ============================================================
# ä¾†æº 4: Google News RSS â€” è·¨åª’é«”æ–°èèšåˆï¼ˆæœ€å»£è¦†è“‹ï¼‰
# ============================================================
async def _search_google_news(client: httpx.AsyncClient, game_name: str, since: datetime) -> list[dict]:
    """é€é Google News RSS æœå°‹éŠæˆ²ç›¸é—œè¡ŒéŠ·æ–°èï¼ˆä¸éœ€ API keyï¼‰"""
    # ç”¨ç´”éŠæˆ²åç¨±æœå°‹ï¼ˆä¸åŠ é—œéµå­—é™åˆ¶ï¼‰ï¼Œè®“ post-filter è™•ç†ç›¸é—œæ€§
    encoded = urllib.parse.quote(f'"{game_name}"')
    url = f"https://news.google.com/rss/search?q={encoded}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"

    try:
        resp = await client.get(url, timeout=15)
        if resp.status_code != 200:
            return []
    except Exception:
        return []

    feed = feedparser.parse(resp.text)
    results = []

    # éè¡ŒéŠ·å™ªéŸ³é—œéµå­—ï¼ˆæ’é™¤çŠ¯ç½ªæ–°èã€è‚¡å¸‚ã€ç´”é›»ç«¶è³½äº‹ç­‰ç„¡é—œå ±å°ï¼‰
    noise_keywords = [
        "æ€§ä¾µ", "è©é¨™", "çŠ¯ç½ª", "é€®æ•", "åˆ¤åˆ‘", "èµ·è¨´", "é…’é§•",
        "è‚¡åƒ¹", "è²¡å ±", "ç‡Ÿæ”¶å ±å‘Š", "æ³•èªªæœƒ",
    ]

    for entry in feed.entries:
        title = entry.get("title", "")
        link = entry.get("link", "")
        source_name = ""
        if hasattr(entry, "source"):
            source_name = entry.source.get("title", "") if isinstance(entry.source, dict) else str(entry.source)
        pub_str = entry.get("published", "")

        # æ¨™é¡Œå¿…é ˆåŒ…å«éŠæˆ²åç¨±ï¼ˆé˜²æ­¢æ··å…¥å…¶ä»–éŠæˆ²çš„æ–°èï¼‰
        if not _title_contains_game(title, game_name):
            continue

        # è§£ææ—¥æœŸï¼Œéæ¿¾è¶…å‡ºç¯„åœçš„
        pub_dt = None
        try:
            pub_dt = parsedate_to_datetime(pub_str)
            if pub_dt < since:
                continue
        except Exception:
            pass  # ç„¡æ³•è§£ææ—¥æœŸçš„ä»ä¿ç•™

        # æ’é™¤å™ªéŸ³
        if any(kw in title for kw in noise_keywords):
            continue

        # å¿…é ˆåŒ…å«è¡ŒéŠ·ç›¸é—œé—œéµå­—
        marketing_kws = EVENT_KEYWORDS + COLLAB_KEYWORDS + AD_KEYWORDS
        if not any(kw in title for kw in marketing_kws):
            continue

        tags = _classify_item(title)

        # æ¸…ç†æ¨™é¡Œï¼ˆGoogle News æœƒåœ¨æœ«å°¾åŠ  " - ä¾†æºå"ï¼‰
        clean_title = title.rsplit(" - ", 1)[0].strip() if " - " in title else title

        results.append({
            "title": clean_title,
            "url": link,
            "summary": f"ä¾†æºï¼š{source_name}" if source_name else "",
            "source": "Google News",
            "published_at": pub_dt.strftime("%Y-%m-%dT%H:%M:%S") if pub_dt else "",
            "tags": tags,
        })

        if len(results) >= 15:
            break

    return results


# ============================================================
# ä¾†æº 5: Google Custom Search â€” Facebook/IG å®˜æ–¹ç¤¾ç¾¤è²¼æ–‡
# ============================================================
async def _search_social_posts(client: httpx.AsyncClient, game_name: str, since: datetime) -> list[dict]:
    """é€é Google Custom Search API æœå°‹ Facebook/IG å…¬é–‹è²¼æ–‡ï¼ˆå… FB API å¯©æ ¸ï¼‰"""
    api_key = os.getenv("GOOGLE_CSE_KEY", "")
    cx = os.getenv("GOOGLE_CSE_CX", "")
    if not api_key or not cx:
        return []

    results = []
    days_back = (datetime.now(TW_TZ) - since).days

    try:
        resp = await client.get(
            "https://www.googleapis.com/customsearch/v1",
            params={
                "key": api_key,
                "cx": cx,
                "q": f'"{game_name}" æ´»å‹• OR è¯å OR åˆä½œ OR æ›´æ–°',
                "dateRestrict": f"d{days_back}",
                "lr": "lang_zh-TW",
                "num": 10,
            },
            timeout=15,
        )
        if resp.status_code != 200:
            _log(f"[WeeklyDigest] Google CSE error {resp.status_code}")
            return []

        data = resp.json()
    except Exception as e:
        _log(f"[WeeklyDigest] Google CSE request failed: {e}")
        return []

    # éè¡ŒéŠ·å™ªéŸ³
    noise_keywords = [
        "æ€§ä¾µ", "è©é¨™", "çŠ¯ç½ª", "é€®æ•", "åˆ¤åˆ‘", "èµ·è¨´",
        "è²·è³£", "ä»£å„²", "ä»£æ‰“", "å¾µäºº", "æ”¶è³¼",
    ]

    for item in data.get("items", []):
        title = item.get("title", "")
        link = item.get("link", "")
        snippet = item.get("snippet", "")

        # æ¨™é¡Œæˆ–æ‘˜è¦å¿…é ˆåŒ…å«éŠæˆ²åç¨±
        if not _title_contains_game(f"{title} {snippet}", game_name):
            continue

        combined = f"{title} {snippet}"

        # æ’é™¤å™ªéŸ³
        if any(kw in combined for kw in noise_keywords):
            continue

        # å¿…é ˆåŒ…å«è¡ŒéŠ·é—œéµå­—
        marketing_kws = EVENT_KEYWORDS + COLLAB_KEYWORDS + AD_KEYWORDS
        if not any(kw in combined for kw in marketing_kws):
            continue

        tags = _classify_item(title, snippet)

        # æ¨™è¨˜ä¾†æº
        if "instagram.com" in link:
            source_label = "Instagram"
        elif "facebook.com" in link:
            source_label = "Facebook"
        else:
            source_label = "ç¤¾ç¾¤æœå°‹"

        results.append({
            "title": title,
            "url": link,
            "summary": snippet[:120] if snippet else "",
            "source": source_label,
            "published_at": "",
            "tags": tags,
        })

    return results


# ============================================================
# ä¸»å‡½å¼
# ============================================================
async def fetch_weekly_digest() -> dict:
    """ä¸»å‡½å¼ï¼šç”¢ç”Ÿæ¯å‘¨éŠæˆ²è¡ŒéŠ·æ‘˜è¦"""
    start_time, now = _get_search_range()
    games = await _get_target_games()

    if not games:
        _log("[WeeklyDigest] No target games found, returning cache")
        return _load_cache()

    digest = []

    async with httpx.AsyncClient(timeout=15, follow_redirects=True) as client:
        for game in games:
            name = game["name"]
            bsn = game.get("bsn")
            _log(f"[WeeklyDigest] Searching: {name} (bsn={bsn})")

            # ä¾†æº 1: Google News RSSï¼ˆæœ€å»£è¦†è“‹ï¼‰
            gnews_items = await _search_google_news(client, name, start_time)

            # ä¾†æº 2: Facebook/IG å®˜æ–¹ç¤¾ç¾¤ï¼ˆBing æœå°‹å…¬é–‹è²¼æ–‡ï¼‰
            fb_items = await _search_social_posts(client, name, start_time)

            # ä¾†æº 3: 4Gamers tag æœå°‹
            fgamers_items = await _search_4gamers(client, name, start_time)

            # ä¾†æº 4: YouTube å®˜æ–¹å½±éŸ³
            yt_items = await _search_youtube(client, name, start_time)

            # ä¾†æº 5: å·´å“ˆéŠæˆ²æ¿æ´»å‹•å…¬å‘Š
            baha_items = await _search_bahamut_board(client, bsn, name)

            all_items = gnews_items + fb_items + fgamers_items + yt_items + baha_items

            if not all_items:
                continue

            # è·¨ä¾†æºå»é‡ï¼ˆç”¨æ¨™é¡Œç›¸ä¼¼åº¦ï¼‰
            all_items = _dedup_items(all_items)

            # åªä¿ç•™æœ‰è¡ŒéŠ·æ¨™ç±¤çš„é …ç›®ï¼ˆç§»é™¤ç´” "news" åˆ†é¡ï¼‰
            all_items = [item for item in all_items
                         if item.get("tags") != ["news"]]

            if not all_items:
                continue

            # æŒ‰ç™¼ä½ˆæ™‚é–“æ’åºï¼ˆç„¡æ™‚é–“çš„æ’æœ€å¾Œï¼‰
            all_items.sort(key=lambda x: x.get("published_at") or "0000", reverse=True)

            # åˆ†é¡çµ±è¨ˆ
            tag_counts = {"ad": 0, "collab": 0, "event": 0, "news": 0}
            for item in all_items:
                for t in item.get("tags", []):
                    tag_counts[t] = tag_counts.get(t, 0) + 1

            digest.append({
                "game": name,
                "source": game["source"],
                "rank": game["rank"],
                "items": all_items,
                "item_count": len(all_items),
                "tag_counts": tag_counts,
                "sources_used": {
                    "google_news": len(gnews_items),
                    "facebook": len(fb_items),
                    "4gamers": len(fgamers_items),
                    "youtube": len(yt_items),
                    "bahamut": len(baha_items),
                },
            })

    # æŒ‰æ¶ˆæ¯æ•¸é‡æ’åºï¼ˆè¡ŒéŠ·æ´»èºåº¦é«˜çš„æ’å‰é¢ï¼‰
    digest.sort(key=lambda x: x["item_count"], reverse=True)

    result = {
        "digest": digest,
        "game_count": len(digest),
        "total_items": sum(g["item_count"] for g in digest),
        "period": {
            "start": start_time.strftime("%Y-%m-%d"),
            "end": now.strftime("%Y-%m-%d"),
        },
        "updated_at": int(time.time()),
    }

    _save_cache(result)
    _log(f"[WeeklyDigest] Done â€” {len(digest)} games, {result['total_items']} total items")
    return result


def _dedup_items(items: list[dict]) -> list[dict]:
    """è·¨ä¾†æºå»é‡ï¼šç›¸åŒæ¨™é¡Œï¼ˆå‰ 20 å­—ï¼‰è¦–ç‚ºé‡è¤‡"""
    seen = set()
    unique = []
    for item in items:
        key = item.get("title", "")[:20]
        if key and key not in seen:
            seen.add(key)
            unique.append(item)
    return unique


def _save_cache(data):
    os.makedirs(CACHE_DIR, exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def _load_cache():
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {"digest": [], "game_count": 0, "total_items": 0}
