"""
æ¯å‘¨éŠæˆ²è¡ŒéŠ·æ‘˜è¦æ¨¡çµ„
- ç›®æ¨™éŠæˆ²ï¼šAndroid ç‡Ÿæ”¶ Top 10 + å·´å“ˆç†±é–€ç‰ˆ Top 10ï¼ˆåˆä½µå»é‡ï¼‰
- è³‡æ–™ä¾†æºï¼š4Gamers tag æœå°‹ + YouTube Data API + å·´å“ˆéŠæˆ²æ¿å…¬å‘Š
- åˆ†é¡ï¼šğŸ“¢ å»£å‘Š/è¡ŒéŠ· â”‚ ğŸ‰ æ´»å‹• â”‚ ğŸ¤ è¯ååˆä½œ
- æ™‚é–“ç¯„åœï¼šéå» 14 å¤©ï¼ˆæ¶µè“‹é€²è¡Œä¸­æ´»å‹•ï¼‰
- æ’ç¨‹ï¼šæ¯å‘¨ä¸€åŸ·è¡Œä¸€æ¬¡
"""
import httpx
from bs4 import BeautifulSoup
import json
import os
import sys
import time
import re
import urllib.parse
from datetime import datetime, timedelta, timezone

TW_TZ = timezone(timedelta(hours=8))


def _log(msg: str):
    """Safe print for Windows cp950 terminal"""
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode(sys.stdout.encoding or "utf-8", errors="replace").decode(
            sys.stdout.encoding or "utf-8", errors="replace"))


CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "cache")
CACHE_FILE = os.path.join(CACHE_DIR, "weekly_digest.json")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8",
}

# â”€â”€ åˆ†é¡é—œéµå­— â”€â”€
EVENT_KEYWORDS = [
    "æ´»å‹•", "é™å®š", "é–‹è·‘", "ç™»å ´", "é–‹æ”¾", "æ›´æ–°", "æ”¹ç‰ˆ", "ç‰ˆæœ¬", "è³½å­£",
    "ç¯€æ…¶", "å‘¨å¹´", "æ˜¥ç¯€", "éå¹´", "æ–°å¹´", "ç¶­è­·", "å…¬å‘Š", "çå‹µ", "å„²å€¼",
    "è½‰è›‹", "æŠ½ç", "å…è²»", "è´ˆé€",
]
COLLAB_KEYWORDS = ["åˆä½œ", "è¯å", "è¯å‹•", "è·¨ç•Œ", "x ", "Ã—", "æ”œæ‰‹", "æˆæ¬Š"]
AD_KEYWORDS = [
    "å»£å‘Š", "ä»£è¨€", "å¤§ä½¿", "å®£å‚³", "PV", "CM", "é å‘Š", "trailer",
    "MV", "å½¢è±¡", "å“ç‰Œ", "å®˜æ–¹", "ä¸»é¡Œæ›²",
]

# â”€â”€ 4Gamers tag åç¨±å°ç…§è¡¨ â”€â”€
TAG_ALIASES = {
    "å‹åˆ©å¥³ç¥ï¼šå¦®å§¬": ["NIKKE", "å‹åˆ©å¥³ç¥"],
    "å´©å£ï¼šæ˜Ÿç©¹éµé“": ["æ˜Ÿç©¹éµé“", "å´©å£æ˜Ÿç©¹éµé“"],
    "è”šè—æª”æ¡ˆ Blue Archive": ["è”šè—æª”æ¡ˆ", "Blue Archive"],
    "Fate/Grand Order": ["FGO", "Fate"],
    "å“ˆåˆ©æ³¢ç‰¹ï¼šé­”æ³•è¦ºé†’": ["å“ˆåˆ©æ³¢ç‰¹"],
    "æ˜æ—¥æ–¹èˆŸï¼šçµ‚æœ«åœ°": ["æ˜æ—¥æ–¹èˆŸ", "Arknights"],
    "å‚³èªªå°æ±º": ["AOV", "Arena of Valor"],
    "å¤©å ‚W": ["å¤©å ‚", "Lineage"],
    "åŸç¥": ["Genshin", "Genshin Impact"],
}


def _classify_item(title: str, summary: str = "") -> list[str]:
    """æ ¹æ“šæ¨™é¡Œå’Œæ‘˜è¦åˆ†é¡æ¶ˆæ¯é¡å‹"""
    text = f"{title} {summary}".lower()
    tags = []
    if any(kw in text for kw in AD_KEYWORDS):
        tags.append("ad")
    if any(kw in text for kw in COLLAB_KEYWORDS):
        tags.append("collab")
    if any(kw in text for kw in EVENT_KEYWORDS):
        tags.append("event")
    if not tags:
        tags.append("news")
    return tags


def _get_search_range():
    """å–å¾—æœå°‹æ™‚é–“ç¯„åœï¼šéå» 14 å¤©ï¼ˆæ¶µè“‹é€²è¡Œä¸­çš„æ´»å‹•ï¼‰"""
    now = datetime.now(TW_TZ)
    start = now - timedelta(days=14)
    return start, now


async def _get_target_games() -> list[dict]:
    """
    å¾ç¾æœ‰å¿«å–å–å¾—ç›®æ¨™éŠæˆ²æ¸…å–®ï¼š
    - Android ç‡Ÿæ”¶ Top 10
    - å·´å“ˆå§†ç‰¹ç†±é–€ç‰ˆ Top 10ï¼ˆå« bsnï¼‰
    åˆä½µå»é‡å¾Œå›å‚³
    """
    games = []
    seen_names = set()

    # 1. Android ç‡Ÿæ”¶ Top 10
    mobile_cache = os.path.join(CACHE_DIR, "mobile_data.json")
    try:
        with open(mobile_cache, "r", encoding="utf-8") as f:
            mobile_data = json.load(f)
        android_grossing = mobile_data.get("android", {}).get("grossing", [])
        for item in android_grossing[:10]:
            name = item.get("name", "").strip()
            if name and name not in seen_names:
                seen_names.add(name)
                games.append({
                    "name": name,
                    "source": "android_grossing",
                    "rank": item.get("rank", 0),
                    "bsn": None,
                })
    except (FileNotFoundError, json.JSONDecodeError, KeyError):
        _log("[WeeklyDigest] Mobile cache not found, skipping Android")

    # 2. å·´å“ˆå§†ç‰¹ç†±é–€ç‰ˆ Top 10ï¼ˆå« bsnï¼‰
    discussion_cache = os.path.join(CACHE_DIR, "discussion_data.json")
    try:
        with open(discussion_cache, "r", encoding="utf-8") as f:
            disc_data = json.load(f)
        bahamut_boards = disc_data.get("bahamut_boards", [])

        # å»ºç«‹ bsn å°ç…§è¡¨ï¼Œä¹Ÿå˜—è©¦å¹« Android éŠæˆ²è£œä¸Š bsn
        bsn_map = {b.get("name", ""): b.get("bsn", "") for b in bahamut_boards}

        for item in bahamut_boards[:10]:
            name = item.get("name", "").strip()
            if name and name not in seen_names:
                seen_names.add(name)
                games.append({
                    "name": name,
                    "source": "bahamut_hot",
                    "rank": item.get("rank", 0),
                    "bsn": item.get("bsn"),
                })

        # å¹«å·²æœ‰çš„ Android éŠæˆ²è£œ bsnï¼ˆåç¨±æ¨¡ç³ŠåŒ¹é…ï¼‰
        for game in games:
            if game["bsn"] is None:
                for board_name, bsn in bsn_map.items():
                    if game["name"] in board_name or board_name in game["name"]:
                        game["bsn"] = bsn
                        break
    except (FileNotFoundError, json.JSONDecodeError, KeyError):
        _log("[WeeklyDigest] Discussion cache not found, skipping Bahamut")

    _log(f"[WeeklyDigest] Target games: {len(games)} "
         f"({len([g for g in games if g['source'] == 'android_grossing'])} Android + "
         f"{len([g for g in games if g['source'] == 'bahamut_hot'])} Bahamut)")
    return games


def _get_tag_variants(game_name: str) -> list[str]:
    """å–å¾—éŠæˆ²åç¨±çš„æ‰€æœ‰å¯èƒ½ tag è®Šé«”"""
    variants = [game_name]
    for canonical, aliases in TAG_ALIASES.items():
        if game_name == canonical or game_name in aliases:
            variants = [canonical] + aliases
            break
    return list(set(variants))


# ============================================================
# ä¾†æº 1: 4Gamers tag æœå°‹
# ============================================================
async def _search_4gamers(client: httpx.AsyncClient, game_name: str, since: datetime) -> list[dict]:
    """æœå°‹ 4Gamers ç‰¹å®šéŠæˆ²çš„è¿‘æœŸæ–°è"""
    items = []
    variants = _get_tag_variants(game_name)

    for tag in variants:
        encoded = urllib.parse.quote(tag)
        url = f"https://www.4gamers.com.tw/site/api/news/by-tag?tag={encoded}&pageSize=20"
        try:
            resp = await client.get(url, headers=HEADERS)
            if resp.status_code != 200 or "json" not in resp.headers.get("content-type", ""):
                continue
            data = resp.json()
            items = data.get("data", {}).get("results", [])
            if items:
                break
        except Exception:
            continue
    else:
        return []

    since_ts = int(since.timestamp() * 1000)
    results = []
    for item in items:
        ts = item.get("createPublishedAt", 0)
        if ts < since_ts:
            continue
        title = item.get("title", "")
        intro = item.get("intro", "") or ""
        results.append({
            "title": title,
            "url": item.get("canonicalUrl", ""),
            "summary": intro[:120],
            "source": "4Gamers",
            "published_at": time.strftime("%Y-%m-%dT%H:%M:%S", time.gmtime(ts / 1000)),
            "tags": _classify_item(title, intro),
        })

    return results


# ============================================================
# ä¾†æº 2: YouTube Data API â€” å®˜æ–¹å½±éŸ³/å»£å‘Š/PV
# ============================================================
async def _search_youtube(client: httpx.AsyncClient, game_name: str, since: datetime) -> list[dict]:
    """æœå°‹ YouTube ä¸Šçš„éŠæˆ²å®˜æ–¹å½±éŸ³/å»£å‘Š"""
    api_key = os.getenv("YOUTUBE_API_KEY", "")
    if not api_key:
        return []

    results = []
    queries = [
        f"{game_name} å®˜æ–¹",
        f"{game_name} å»£å‘Š PV trailer",
    ]
    published_after = since.strftime("%Y-%m-%dT%H:%M:%SZ")

    for q in queries:
        try:
            resp = await client.get(
                "https://www.googleapis.com/youtube/v3/search",
                params={
                    "part": "snippet",
                    "q": q,
                    "type": "video",
                    "publishedAfter": published_after,
                    "regionCode": "TW",
                    "relevanceLanguage": "zh-Hant",
                    "maxResults": 5,
                    "order": "date",
                    "key": api_key,
                },
                timeout=15,
            )
            if resp.status_code != 200:
                _log(f"[WeeklyDigest] YouTube API error {resp.status_code} for '{q}'")
                continue
            data = resp.json()

            for item in data.get("items", []):
                snippet = item.get("snippet", {})
                title = snippet.get("title", "")
                channel = snippet.get("channelTitle", "")
                video_id = item.get("id", {}).get("videoId", "")
                published = snippet.get("publishedAt", "")

                # æ’é™¤ç´”å¯¦æ³/æ”»ç•¥
                skip_words = ["å¯¦æ³", "ç›´æ’­", "æ”»ç•¥", "æ•™å­¸", "é–‹ç®±", "gameplay", "walkthrough", "let's play"]
                if any(sw in title.lower() for sw in skip_words):
                    continue

                results.append({
                    "title": title,
                    "url": f"https://www.youtube.com/watch?v={video_id}",
                    "summary": f"é »é“ï¼š{channel}",
                    "source": "YouTube",
                    "published_at": published[:19] if published else "",
                    "tags": _classify_item(title),
                    "thumbnail": snippet.get("thumbnails", {}).get("medium", {}).get("url", ""),
                })
        except Exception as e:
            _log(f"[WeeklyDigest] YouTube search error: {e}")

    # å»é‡
    seen = set()
    unique = []
    for r in results:
        if r["url"] not in seen:
            seen.add(r["url"])
            unique.append(r)
    return unique


# ============================================================
# ä¾†æº 3: å·´å“ˆå§†ç‰¹éŠæˆ²æ¿ â€” æ´»å‹•/å…¬å‘Š/å®˜æ–¹è²¼æ–‡
# ============================================================
async def _search_bahamut_board(client: httpx.AsyncClient, bsn: str, game_name: str) -> list[dict]:
    """æœå°‹å·´å“ˆå§†ç‰¹éŠæˆ²æ¿ä¸Šçš„æ´»å‹•/å…¬å‘Š/å®˜æ–¹ç›¸é—œè²¼æ–‡"""
    if not bsn:
        return []

    url = f"https://forum.gamer.com.tw/B.php?bsn={bsn}"
    try:
        resp = await client.get(url, headers=HEADERS)
        if resp.status_code != 200:
            return []

        soup = BeautifulSoup(resp.text, "html.parser")
        results = []
        seen_titles = set()

        # æ´»å‹•/å…¬å‘Šç›¸é—œé—œéµå­—
        board_event_kws = [
            "æ´»å‹•", "å…¬å‘Š", "å®˜æ–¹", "æ›´æ–°", "ç¶­è­·", "è¯å", "åˆä½œ", "é™å®š",
            "é–‹è·‘", "çå‹µ", "å…è²»", "è´ˆé€", "é å‘Š", "æ–°ç‰ˆ", "æ”¹ç‰ˆ", "è³½äº‹",
        ]

        for a in soup.select("a[href]"):
            href = a.get("href", "")
            if "C.php?bsn=" not in href:
                continue

            title = a.get_text(strip=True)
            if not title or len(title) < 5 or title in seen_titles:
                continue

            # ç¯©é¸æ´»å‹•/å…¬å‘Šç›¸é—œ
            if not any(kw in title for kw in board_event_kws):
                continue

            seen_titles.add(title)
            if not href.startswith("http"):
                href = f"https://forum.gamer.com.tw/{href}"

            results.append({
                "title": title,
                "url": href,
                "summary": f"å·´å“ˆ {game_name} æ¿",
                "source": "å·´å“ˆè¨è«–æ¿",
                "published_at": "",  # å·´å“ˆæ¿æ–‡æ™‚é–“è¼ƒé›£å–å¾—
                "tags": _classify_item(title),
            })

            if len(results) >= 10:
                break

        return results
    except Exception as e:
        _log(f"[WeeklyDigest] Bahamut board error for bsn={bsn}: {e}")
        return []


# ============================================================
# ä¸»å‡½å¼
# ============================================================
async def fetch_weekly_digest() -> dict:
    """ä¸»å‡½å¼ï¼šç”¢ç”Ÿæ¯å‘¨éŠæˆ²è¡ŒéŠ·æ‘˜è¦"""
    start_time, now = _get_search_range()
    games = await _get_target_games()

    if not games:
        _log("[WeeklyDigest] No target games found, returning cache")
        return _load_cache()

    digest = []

    async with httpx.AsyncClient(timeout=15, follow_redirects=True) as client:
        for game in games:
            name = game["name"]
            bsn = game.get("bsn")
            _log(f"[WeeklyDigest] Searching: {name} (bsn={bsn})")

            # ä¾†æº 1: 4Gamers tag æœå°‹
            fgamers_items = await _search_4gamers(client, name, start_time)

            # ä¾†æº 2: YouTube å®˜æ–¹å½±éŸ³
            yt_items = await _search_youtube(client, name, start_time)

            # ä¾†æº 3: å·´å“ˆéŠæˆ²æ¿æ´»å‹•å…¬å‘Š
            baha_items = await _search_bahamut_board(client, bsn, name)

            all_items = fgamers_items + yt_items + baha_items

            if not all_items:
                continue

            # è·¨ä¾†æºå»é‡ï¼ˆç”¨æ¨™é¡Œç›¸ä¼¼åº¦ï¼‰
            all_items = _dedup_items(all_items)

            # æŒ‰ç™¼ä½ˆæ™‚é–“æ’åºï¼ˆç„¡æ™‚é–“çš„æ’æœ€å¾Œï¼‰
            all_items.sort(key=lambda x: x.get("published_at") or "0000", reverse=True)

            # åˆ†é¡çµ±è¨ˆ
            tag_counts = {"ad": 0, "collab": 0, "event": 0, "news": 0}
            for item in all_items:
                for t in item.get("tags", []):
                    tag_counts[t] = tag_counts.get(t, 0) + 1

            digest.append({
                "game": name,
                "source": game["source"],
                "rank": game["rank"],
                "items": all_items,
                "item_count": len(all_items),
                "tag_counts": tag_counts,
                "sources_used": {
                    "4gamers": len(fgamers_items),
                    "youtube": len(yt_items),
                    "bahamut": len(baha_items),
                },
            })

    # æŒ‰æ¶ˆæ¯æ•¸é‡æ’åºï¼ˆè¡ŒéŠ·æ´»èºåº¦é«˜çš„æ’å‰é¢ï¼‰
    digest.sort(key=lambda x: x["item_count"], reverse=True)

    result = {
        "digest": digest,
        "game_count": len(digest),
        "total_items": sum(g["item_count"] for g in digest),
        "period": {
            "start": start_time.strftime("%Y-%m-%d"),
            "end": now.strftime("%Y-%m-%d"),
        },
        "updated_at": int(time.time()),
    }

    _save_cache(result)
    _log(f"[WeeklyDigest] Done â€” {len(digest)} games, {result['total_items']} total items")
    return result


def _dedup_items(items: list[dict]) -> list[dict]:
    """è·¨ä¾†æºå»é‡ï¼šç›¸åŒæ¨™é¡Œï¼ˆå‰ 20 å­—ï¼‰è¦–ç‚ºé‡è¤‡"""
    seen = set()
    unique = []
    for item in items:
        key = item.get("title", "")[:20]
        if key and key not in seen:
            seen.add(key)
            unique.append(item)
    return unique


def _save_cache(data):
    os.makedirs(CACHE_DIR, exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def _load_cache():
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {"digest": [], "game_count": 0, "total_items": 0}
